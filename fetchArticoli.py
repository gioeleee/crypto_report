import sqlite3
import time
import tempfile
from selenium import webdriver
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from bs4 import BeautifulSoup
from webdriver_manager.chrome import ChromeDriverManager
import database
import utilities

BASE_URL = "https://cryptopanic.com"



def setup_chrome_driver():
    """Crea e restituisce un'istanza di Chrome WebDriver configurata."""
    chrome_options = Options()
    #chrome_options.add_argument("--headless=new")  # Attivalo se serve in produzione
    chrome_options.add_argument("--disable-gpu")
    chrome_options.add_argument("--no-sandbox")
    chrome_options.add_argument("user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64)")
    #chrome_options.add_argument(f"--user-data-dir={tempfile.mkdtemp()}")

    return webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=chrome_options)



def scroll_news_page(driver, scroll_container, max_pause=2):
    """Esegue lo scroll e clicca 'Load more' fino a quando non ci sono pi√π nuovi articoli."""
    articles_seen = 0
    scroll_attempts = 0

    while True:
        driver.execute_script("arguments[0].scrollTop = arguments[0].scrollHeight", scroll_container)
        time.sleep(max_pause)

        try:
            load_more_button = driver.find_element(By.XPATH, "//button[contains(@class, 'btn-outline-primary')]")
            if load_more_button.is_displayed():
                print("üîΩ Bottone 'Load more' trovato. Clicco...")
                driver.execute_script("arguments[0].click();", load_more_button)
                time.sleep(max_pause)
        except:
            pass

        soup = BeautifulSoup(driver.page_source, "html.parser")
        news_divs = soup.find_all("div", class_="news-row news-row-link")
        current_count = len(news_divs)

        print(f"üåÄ Scroll {scroll_attempts+1}: {current_count} articoli trovati finora")

        if current_count == articles_seen:
            print("üõë Scroll terminato: nessun nuovo contenuto caricato.")
            break

        articles_seen = current_count
        scroll_attempts += 1


def estrai_articoli_da_soup(soup):
    """Estrae lista di tuple (url, data, titolo) da un oggetto BeautifulSoup."""
    articoli = []

    for div in soup.find_all("div", class_="news-row news-row-link"):
        a_tag = div.find("a", href=True)
        time_tag = div.find("time", datetime=True)
        title_span = div.find("span", class_="title-text")
        title = None

        if title_span:
            inner_span = title_span.find("span")
            if inner_span:
                title = inner_span.get_text(strip=True)

        if not a_tag:
            continue

        relative_url = a_tag["href"]
        full_url = BASE_URL + relative_url
        published_at_raw = time_tag["datetime"] if time_tag else None
        published_at = utilities.convert_to_sql_datetime(published_at_raw) if published_at_raw else None

        articoli.append((full_url, published_at, title))

    return articoli


def ordina_articoli_per_data(articoli):
    """Ordina la lista di articoli per data crescente (dal pi√π vecchio)."""
    return sorted(
        [a for a in articoli if a[1] is not None],
        key=lambda x: x[1]
    )


def seleziona_feed_italiano(driver):
    try:
        # 1. Passa sopra l'elemento "Region" per mostrare il menu
        region_menu = WebDriverWait(driver, 10).until(
            EC.presence_of_element_located((By.CSS_SELECTOR, ".region-switch"))
        )
        driver.execute_script("arguments[0].scrollIntoView(true);", region_menu)
        webdriver.ActionChains(driver).move_to_element(region_menu).perform()
        time.sleep(1.5)

        # 2. Trova e deseleziona "English" se √® attivo
        english_li = driver.find_elements(By.XPATH, "//li[contains(text(), 'English') and contains(@class, 'active')]")
        if english_li:
            print("üåê Deseleziono English...")
            driver.execute_script("arguments[0].click();", english_li[0])
            time.sleep(1)

        # 3. Assicura che "Italiano" sia attivo
        italiano_li = driver.find_elements(By.XPATH, "//li[contains(text(), 'Italiano') and not(contains(@class, 'active'))]")
        if italiano_li:
            print("üáÆüáπ Attivo Italiano...")
            driver.execute_script("arguments[0].click();", italiano_li[0])
            time.sleep(1)

        print("‚úÖ Feed impostato su solo Italiano.")
    except Exception as e:
        print(f"‚ùå Errore nel filtrare solo italiano: {e}")






### Funzione per estrarre gli articoli da cryptopanic, estrate titolo, url_cryptopanic e data
def fetchArticoliCryptopanic():
    driver = setup_chrome_driver()
    driver.get(f"{BASE_URL}/news")

    # üëá Disattiva filtro inglese se attivo
    seleziona_feed_italiano(driver)

    WebDriverWait(driver, 15).until(
        EC.presence_of_element_located((By.CLASS_NAME, "news-row"))
    )
    print("‚úÖ Pagina caricata, articoli visibili.")

    scroll_container = WebDriverWait(driver, 10).until(
        EC.presence_of_element_located((By.CLASS_NAME, "news-container"))
    )

    scroll_news_page(driver, scroll_container)

    soup = BeautifulSoup(driver.page_source, "html.parser")
    driver.quit()

    articoli = estrai_articoli_da_soup(soup)
    articoli_ordinati = ordina_articoli_per_data(articoli)

    database.salvaArticoliCryptopanic(articoli_ordinati)



def get_original_url(cryptopanic_url):
    chrome_options = Options()
    # chrome_options.add_argument("--headless=new")  # Abilita in produzione
    chrome_options.add_argument("--disable-gpu")
    chrome_options.add_argument("--no-sandbox")
    chrome_options.add_argument("user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64)")
    chrome_options.add_argument(f"--user-data-dir={tempfile.mkdtemp()}")

    for attempt in range(3):
        try:
            driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=chrome_options)
            driver.get(cryptopanic_url)

            # Attendi che il link al titolo sia cliccabile
            WebDriverWait(driver, 10).until(
                EC.element_to_be_clickable((By.CSS_SELECTOR, "h1.post-title span.text"))
            )

            article_title = driver.find_element(By.CSS_SELECTOR, "h1.post-title span.text")
            print(f"üì∞ Titolo trovato: {article_title.text}")

            # Scroll e click tramite JS
            driver.execute_script("arguments[0].scrollIntoView(true);", article_title)
            time.sleep(0.5)
            driver.execute_script("arguments[0].click();", article_title)
            time.sleep(2)

            # Passa alla nuova scheda
            if len(driver.window_handles) > 1:
                driver.switch_to.window(driver.window_handles[1])
                time.sleep(2)

            return driver.current_url

        except Exception as e:
            print(f"‚ùå Errore al tentativo {attempt+1} per {cryptopanic_url}: {e}")
        finally:
            try:
                driver.quit()
            except:
                pass

        time.sleep(2)

    return None



### Funzione per estrarre gli url originali degli articoli
def fetchUrlArticoli():
    articoli = database.get_articoli_senza_url_originale()

    if not articoli:
        print("‚úÖ Nessun articolo da aggiornare.")
        return

    print(f"üîç Trovati {len(articoli)} articoli da elaborare.")

    for id_articolo, url_cryptopanic in articoli:
        print(f"\nüîÑ Elaborazione articolo ID {id_articolo}...")

        url_articolo = get_original_url(url_cryptopanic)

        if url_articolo:
            print(f"‚úÖ URL originale trovato: {url_articolo}")
            database.aggiorna_url_originale(id_articolo, url_articolo)
        else:
            print(f"‚ö†Ô∏è Nessun URL trovato per {url_cryptopanic}")
            database.aggiorna_url_originale(id_articolo, "NESSUN CONTENUTO")

        time.sleep(1.5)  # Rate limiting


def fetch_html_content(url):
    chrome_options = Options()
    chrome_options.add_argument("--headless=new")
    chrome_options.add_argument("--disable-gpu")
    chrome_options.add_argument("--no-sandbox")
    chrome_options.add_argument("user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64)")
    chrome_options.add_argument(f"--user-data-dir={tempfile.mkdtemp()}")

    driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=chrome_options)

    try:
        driver.get(url)
        WebDriverWait(driver, 15).until(
            EC.presence_of_element_located((By.TAG_NAME, "body"))
        )
        return driver.page_source

    except Exception as e:
        print(f"‚ùå Errore durante il download HTML da {url}: {e}")
        return None

    finally:
        driver.quit()



def fetch_and_process_html_articoli():
    from database import get_articoli_da_processare_html, salva_html_articolo
    from cleanHtml import clean_html_content

    articoli = get_articoli_da_processare_html()
    if not articoli:
        print("‚úÖ Tutti gli articoli sono gi√† stati elaborati.")
        return

    for id_articolo, url_articolo in articoli:
        print(f"\nüîç Elaborazione articolo ID {id_articolo}")

        html = fetch_html_content(url_articolo)
        if not html:
            print("‚ö†Ô∏è HTML non disponibile.")
            continue

        pulito = clean_html_content(html)
        if not pulito.strip():
            print("‚ö†Ô∏è HTML pulito vuoto o non valido.")
            continue

        try:
            salva_html_articolo(id_articolo, pulito)
            print(f"‚úÖ HTML salvato per articolo ID {id_articolo}")
        except Exception as e:
            print(f"‚ùå Errore nel salvataggio DB per articolo ID {id_articolo}: {e}")

        time.sleep(1.5)  # rate limit
